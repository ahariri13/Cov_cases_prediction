# -*- coding: utf-8 -*-
"""GCN_Testing_May11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxsCQ7IhpyRqCDhNK9TXbBNZBHAKjxrl
"""

"""
Here we try GGN LSTM

"""
#%%
from __future__ import division
from __future__ import print_function
import argparse
import time
import torch_geometric
import numpy as np
import scipy.sparse as sp
import torch
from torch import optim
from torch_geometric.data import Data
from may21_model_MPNN import GCNModelVAE
import torch.nn.functional as F
import time 
import torch.nn as nn
from torch.utils.data import Dataset, IterableDataset, DataLoader
import torch_geometric.utils
use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")


parser = argparse.ArgumentParser()
parser.add_argument('--model', type=str, default='gcn_vae', help="models used")
parser.add_argument('--seed', type=int, default=42, help='Random seed.')
parser.add_argument('--epochs', type=int, default=50, help='Number of epochs to train.')
parser.add_argument('--batch_size', type=int, default=3, help='Initial learning rate.') #100
parser.add_argument('--hidden1', type=int, default=128, help='Number of units in hidden layer 1.') #256
parser.add_argument('--hidden2', type=int, default=64, help='Number of units in hidden layer 2.') #128
parser.add_argument('--lr', type=float, default=0.0001, help='Initial learning rate.') #0.001
parser.add_argument('--dropout', type=float, default=0.3, help='Dropout rate (1 - keep probability).')
args = parser.parse_args([])
torch.backends.cudnn.benchmark = True


out_channels=500
num_layers=2
agg='mean'
input_feat_dim=7
hidden_dim1=50
hidden_dim2=15

model = GCNModelVAE(args.batch_size,input_feat_dim, hidden_dim1,hidden_dim2,args.dropout,out_channels,num_layers)
optimizer = optim.Adam(model.parameters(), lr=args.lr)
#optimizer = optim.RMSprop(model.parameters(), lr=args.lr,weight_decay=0.001,eps=1e-06)
model.train()
model.to(device)

loss_function = nn.MSELoss()


def clean(a2):
    aref=a2.clone()
    aref=torch.transpose(aref,0,2)
    aref=torch.transpose(aref,1,2)
    #aref[aref != aref] = 0
    #aref=torch.sum(aref,dim=2)
    return aref 


def windowsFeat(inputD,slide):  ### slice feat periods to windows of size 'slide'to correspond to timeframe 
    tens=inputD.clone()#clean(inputD)
    inout_seq=[]
    train_seq=[]
    train_label=[]
    L=tens.shape[0] ##57 
    for i in range(L):
        end_ix = i + slide
        if end_ix>=L:
            break
        train_seq.append(tens[i:end_ix,:,:].numpy()) ## 1,336
        train_label.append(tens[end_ix,:,-1:]) ## 48,1
        #inout_seq.append((train_seq.float(),train_label))
    return torch.tensor(np.array(train_seq), dtype=float, requires_grad=True),torch.stack(train_label)

#%%
def load_data(data):
    states=torch.load(data)
    
    a2=states['x1']
    e=states['edge_index']
    
    check=a2.numpy()  ## Features matrix 
    c=torch.from_numpy(check).clone()
    c=clean(c) ## Permute it to shape (57,48,7)
    
    me=[]
    for i in range(c.shape[0]):
        val,ind=abs(c[i]).max(-2)  ### Append the 7 max values , i.e the max value of each feature column
        me.append(val.numpy())
    me=np.array(me)
    
    
    top=np.max(me[:,6],axis=0)  ## Get max value of # of un-normalized cases 
    
    up_a=[]
    a3=c.clone()
    
    ### Normalize the features by dividing by maximum value.
    for k in range(len(me)):
        tempM=torch.stack([torch.tensor(me[k,:]) for i in range(48)])
        a3[k][:,:-1]/=tempM[:,:-1]#/temp#%%
        a3[k][:,-1:]/=400#top
        up_a.append(a3[k])
    
    
    up_a=torch.stack(up_a)
    statsX,statsY=windowsFeat(up_a,args.batch_size)
    
    adj=torch_geometric.utils.to_dense_adj(e)
    adj2=[]
    for k in range(args.batch_size):
        adj2.append(adj)
    adj2=torch.stack(adj2)

    statsY=statsY.squeeze_(2).float()
    
    adj2=adj2.squeeze_(1).float()
    
    return statsX,statsY,adj2,e

#%%

statsX,statsY,adj2,e=load_data('zeroed_themap_onetens.pt')
#%%
states=torch.load('zeroed_themap_onetens.pt')

a2=states['x1']
e=states['edge_index']

#%%

def cutLoss(r1,labels):
    loss2=0
    for k in range(labels.shape[0]):
        #labels[k][labels[k]==0]=0.0001
        loss2+=(abs(r1[k]-labels[k])/(1+abs(r1[k]))).mean()
            #loss+=torch.tensor(1)#abs(r1[k]-labels[k])
    #loss3=loss2.numpy()
    return loss2/(r1.shape[0])
#labs=torch.stack([kk[1] for kk in stats])
#labs=labs.squeeze_(2)

#statsX=statsX.float()
#%%
   check= torch.from_numpy(np.zeros((48)))
   #%%
   check.shape
    #%%
statsX=statsX.to(device)
statsY=statsY.to(device)
e=e.to(device)
adj2=adj2.to(device)

eps=[]
count=0
epLoss=0

for epoch in range(300):
        count+=1

        t = time.time()
        optimizer.zero_grad()

        r1= model(a2.float(),adj2,e)#adj.squeeze_(1)) ## X , A 

        #loss=loss_function(r1.float(),statsY)#.sum(dim=0)

        loss=cutLoss(r1.float(),statsY)#.sum(dim=0)
        cur_loss = loss.item()
        
        loss.backward()

        #cur_loss = loss.detach().numpy()#.item()
        optimizer.step()
        epLoss+=cur_loss

        print("Epoch:", '%04d' % (epoch + 1), "train_loss=", "{:.5f}".format(epLoss/count))
        eps.append(epLoss/count)

     # "time=", "{:.5f}".format(time.time() - t))
        t = time.time()

#%%
        
import matplotlib.pyplot as plt
plt.plot(eps)
#%%

statsXTest,statsYTest,adj2Test,eTest=load_data('updated2_themap_onetens.pt')   
#%%
see=statsXTest.detach().numpy()[0]
#%%
sampleTest=statsXTest[57:,:,:,:6].clone()
#%%
samplePred=statsX[-1:,:,:,:].clone()
#%%
sampleTest.shape

states=torch.load('updated2_themap_onetens.pt')

a2=states['x1']
e=states['edge_index']

check3=a2.numpy()  ## Features matrix 
c=torch.from_numpy(check3).clone()
c=clean(c) ## Permute it to shape (57,48,7)

statsX.shape

with torch.no_grad():
  model.eval()
  r2=model(statsX[-4:].float(),adj2.to(device),e.to(device),predLength=12,Testdata=sampleTest.to(device),phase='Test')
  check=r2.detach().cpu().clone().numpy()
  pred=[]
  for m in range(11):
    pred.append(check[m,:]*400) 
  pred2=np.array(pred)
  pred2=np.squeeze(pred2)

  #%%
  import matplotlib.pyplot as plt
  plt.plot(c[57:,7,-1:].numpy())
  print(pred2.shape)
  plt.plot(pred2[:,7])
  #%%

check.shape

check=r2.detach().cpu().clone().numpy()
"""
predM=[]
for k2 in range(len(me)):
    predM.append(me[k2])
"""
#%%
pred=[]
for m in range(11):
    pred.append(check[m,:]*400)

pred2=np.array(pred)
#%%
pred2=np.squeeze(pred2)

pred2.shape

c.shape

pred2=np.transpose(pred2)

pred2.shape

#%%
import matplotlib.pyplot as plt
plt.plot(c[57:,7,-1:].numpy())
plt.plot(pred2[7,:])
#%%

import matplotlib.pyplot as plt
plt.plot(c[:,7,-1:].numpy())
plt.plot(pred2[:,7])

import matplotlib.pyplot as plt
plt.plot(c[:,0,-1:].numpy())
plt.plot(pred2[:,0])

torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            }, './weights.pth')